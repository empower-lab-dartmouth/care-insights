// All time metrics are in milliseconds or epoc/unix time (also milliseconds). For this reason, date and time values are numbers.
// see https://en.wikipedia.org/wiki/Unix_time for more info.


type EngagementLevel = 'low' | 'average' | 'high' | 'none' | 'na'
type RedirectionLevel = 'success' | 'none' | 'unsuccessful' | 'na'


type MeaningfulMoment = {
   startTime: number, // Start of the moment, in milliseconds relative to the video start.
   uuid: string, // Generate a UUID for the meaningful moment object itself so we can distinguish it. use a package like this one: https://www.npmjs.com/package/uuid
   // Note: this uuid should match the key value used in the parent object meaningfulMoments map field (see below)
   description: string, // Describe what is happening. This should be framed in terms of cause and effect. See below. More info to come on this.
   // I expect we identify meaningful moments through two approaches: Using segmentation or exceptional value detection on heatmap data and then generating text descriptions using ChatGPT,
   // or just using chatGPT transcript.
   type: 'positiveMusic' | 'redirection' | 'memoryRecall' // We look for specific types of moments, I'll outline the ones I have here, but more may come.
   // positiveMusic -> a moment when the musical stimuli caused a positive reaction. The description should utilize knowledge of a song event being generated by the Memcara
   // in a context where there was a jump in positive emotion. To further substantiate what is happening, we should utilize the generated factors of engagement heatmap data as well as
   // the transcript. An example of this would be: "'Fly me to the moon' caused a jump in Betty's mood, she remarked: 'I love Frank Sinatra'." We definitely will need ChatGPT to generate this.
   //
   // redirection -> a moment of redirection is when the care recipient is displaying concerning symptoms of dementia (something we predict in our original heatmap data) and then is redirected to
   // stop these behaviors through some kind of stimuli. We can find this through a rules based approach where there is a notable change in behavior. As with postiveMusic, the description should
   // be generated based on known content in the session, as recorded in the transcript and music events. An example of this would be "Betty started displaying wandering behavior, she was redirected through a conversation about her brother."
   //
   // memoryRecall -> a moment when the care recipient shows particularly lucid memory recall. We predict this in heat map data and can leverage that for identifying the segments. Then, transcript and
   // and song event data can be used with ChatGPT to generate a substantive description of what is happening using our stimulus cause -> reaction template. An example would be: 'The caregiver's questions about
   // the sound of the trumpet led Betty to recall how she loved playing tuba in high school band alongside her sister." If the heatmap approach doesn't work, we should target moments relating to family using purely GPT analysis.
   // That can be the backup option if there are not enough meaningful moments identifiable through the first approach via heatmap (i.e. if the heatmap data is all null for some reason).
}


type HeatmapData = {
   dataPointDuration: number, // What is the window that one data point covers. Let's set this to 30 seconds to start with, but adding a field here allows us to be dynamic.
   minValue: number, // Ideally, let's scale everything to a probability, so this would be 0 and max value would be 1, then on the front end I could have a "sensitivity" knob that can be adjusted to nonlinearly change the heatmap display
   maxValue: number,
   factorsOfEngagement: Record<string, number[]> // Specifically, these are "attention", "memory-recall," "positive-emotions," "reaction," "concerning-response"
   // To generate factors of engagement, we'll have to merge existing factors that we are generating.
}


type TranscriptSegment = { // On the frontend, I don't care about what the duration of a transcript segment is, we can choose something reasonable, i.e. 30-60 seconds
   text: string // Transcript text for this segment
   startTime: number // Relative start time of this segment. Used on front end to enable clicking into a particular segment of the video.
}


export type MusicProgramEvent = {
   type: 'music-event', // Hardcode this, on the frontend it's necessary, since we are also allowing for manually generated events, which have a different type.
   videoUrl: string, // This will be the MUX link, not the Google cloud bucket url. Figuring out what this is is a TODO.
   meaningfulMoments: Record<string, MeaningfulMoment>, // A uuid -> meaningfulMoment mapping of the meaningful moments in the video. Let's require 2-6 meaningful moments per video.
   transcript: TranscriptSegment[], // Reduce the transcript to segments that have no repetition.
   label: string, // Use the Memcara program name, i.e. "Sights and sounds music therapy program" this should be defined in the JSON.
   date: number, // Epoc start time of the video
   uuid: string, // Generate a UUID for the event itself so we can distinguish it. use a package like this one: https://www.npmjs.com/package/uuid
   CRUUID: string, // Care recipient's uuid, this value is defined in the JSON
   CGUUID: string, // Caregiver's uuid, this value is defined in the JSON
   description: string, // Reformulate text from the meaningful moments into a single string, either just through concatenation or with merge help from ChatGPT. This representation is necessary for supporting our frontend querying.
   engagement: EngagementLevel, // Predict engagement level of user. Let's do a rules based approach to this for now based on meaningful moments data (i.e. are there lots of positive moments?). This is not a priority. Once we have user labels (from caregivers editing these values) we can retrain on those
   redirection: RedirectionLevel, // were there moments of redirection in the video? Let's do a rules based approach to this for now based on meaningful moments (i.e. is there a redirection moment?) This is not a priority. Once we have user labels (from caregivers editing these values) we can retrain on those
  
}
